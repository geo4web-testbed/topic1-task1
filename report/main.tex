\documentclass[a4paper]{scrartcl}

\usepackage{authblk}

\usepackage[USenglish]{babel}

\usepackage{bibentry}
\usepackage[official]{eurosym}

\usepackage{enumitem}

\usepackage[T1]{fontenc}

\usepackage{graphicx}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}

\usepackage{minted}

\usepackage{verbatim}

% Font corrections.
\newcommand{\textt}[1]{{\small \texttt{#1}}}

% Disable default paragraph indentation.
\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0cm}

\title{Modern Ways of Spatial Data Publication}
\author{Wouter Beek ({\small\url{wouter@triply.cc}})}
\author{Laurens Rietveld ({\small\url{laurens@triply.cc}})}
\affil{Triply ({\small\url{http://triply.cc}})}
\date{v0.2}

\begin{document}

\maketitle

\begin{abstract}
  \noindent Almost every interesting dataset has some spatial
  component.  Besides being prevalent, spatial relations --
  particularly geographical ones -- tie the online to the offline
  world.  As such, they provide a grounding of data stored in
  databases to the physical environment that is described in those
  databases.

  Given the availability of Semantic Web services, Linked Datasets and
  Open Source web libraries we should be able to build a demonstration
  system that allows web programmers to build innovative applications
  on top of integrated Linked (Geo)datasets.  Unfortunately, we found
  out that this is not (yet) the case.
\end{abstract}


\section{Introduction}

Earlier research by GeoNovum has resulted in a collection of lessons
learned that describe in great detail the requirements of a modern
spatial data publishing infrastructure.  The purpose of the present
report is to document our research findings based on this prior work
in an attempt to answer the following research question:

\begin{quote}
  How do the lessons learned meet the constraints (e.g., budgets) and
  capabilities (e.g., in-house know-how) of governmental organizations
  on the one hand, and of data users on the other?
\end{quote}

While every governmental organization will be different, e.g., will be
required to follow different rules and regulations depending on the
domain or context in which it operates, it is possible to quantify the
investment needed in order to build a Linked Geodata platform that
implements the lessons learned.


\begin{comment}
\section{Approach}

Quantification is performed in a four-stage process:

\begin{itemize}

\item Information gathering: Based on Linked Data and Geodata experts
  and available we perfom a quick information gathering round in which
  we form an overview of the directions we can take in order to answer
  our research question.  The information gathered is not inteded to
  be complete but is intended to give a global direction for
  subsequent stages.  A more extensive information gathering approach
  may be undertaken in the future.

\item Scaffolding step: Based on the information gathered, we make an
  assessment of current platforms, libraries and tools that would
  allow the requirements to be implemented.  Also, some of the
  stengths and weaknesses that did not surface during the information
  gathering stage may come up here.

\item Filling in the blanks: Not all requirements may be available
  from off-the-shelf tools.  It may therefore be necessary to develop
  the remaining requirements.  This can either be done as a community
  effort or by the organization itself.  The cost/benefit calculations
  for these two approaches are very different: while the costs of a
  community effort can be spread and are therefore generally lower,
  development may take a while.  On the other hand, in-house
  development is relatively expensive but is likely to result in a
  quick first working solution.  In practice, the challenge is to turn
  components that are developed in-houde for speed into long-running
  community projects afterwards.

\item Test \& iterate: Once an acceptable platform is assembled we
  need to go through several iterations of testing, upgrading and
  testing.  We were not able to reach this fourth step in this
  research.  Section~\ref{sec:conclusion} gives details as to why.

\end{itemize}
\end{comment}


\section{First strategy: use a SotA triple store}

When we started out building our geospatial Linked Data demonstrator
we performed an industry-wide assessment of existing tools.  For this
we have consulted people from multiple domains.  We first determined
the leading query paradigm for geospatial Linked Data: this is
GeoSPARQL~\cite{Battle2011}.  We then determined the tool with the
best GeoSPARQL support: this is Virtuoso\footnote{See
  \url{https://github.com/openlink/virtuoso-opensource}}.  We have
loaded the data into an endpoint hosted at
\url{http://sparql.geonovum.triply.cc/sparql}.

However, even Virtuoso does not support all geometries.  Our data
contains curves, resulting in the following error whenever a curve is
encountered by the query engine:

\begin{minted}{text}
  Virtuoso 42000 Error GEO..: for after check of geo intersects, some
  shape types (e.g., polygon rings and curves) are not yet supported
\end{minted}

Here is an example of a query that gives the above warning (``Pairs of
intersecting geometries that belong to the same resource.''):

\begin{minted}{sparql}
PREFIX geo: <http://www.opengis.net/ont/geosparql#>
SELECT ?y1 ?y2
WHERE {
  ?x geo:asWKT ?y1 .
  ?x geo:asWKT ?y2
  FILTER (bif:st_intersects (?y1, ?y2, 0.1))
}
\end{minted}

A second issue is that some queries do not terminate at all, as
indicated by the following message:

\begin{minted}{text}
  Virtuoso S1T00 Error SR171: Transaction timed out
\end{minted}

An example is an altered version of the above query (``Five pairs of
dissimilar intersecting geometries that belong to the same
resource.''):

\begin{minted}{sparql}
PREFIX geo: <http://www.opengis.net/ont/geosparql#>
SELECT ?y1 ?y2 {
  ?x geo:asWKT ?y1 .
  ?x geo:asWKT ?y2
  FILTER (bif:st_intersects (?y1, ?y2, 0.1) && ?y1 != ?y2)
}
LIMIT 5
\end{minted}

Thirdly, not all combinations of supported functions and supported
shapes are supported.  For instance, the following implements the geo
query for our original use case:

\begin{minted}{sparql}
PREFIX geo: <http://www.opengis.net/ont/geosparql#>
SELECT ?y (MIN(bif:st_distance(?y, bif:st_point(0, 52))) AS ?z)
WHERE {
  ?x1 geo:asWKT ?y
}
LIMIT 5
\end{minted}

In this particular case, the problem is that distance can only be
calculated between points but not between a point and a surface, as
communicated by the following message:

\begin{minted}{text}
Virtuoso 22023 Error GEO..: Function st_distance() expects a geometry
of type 1 as argument 0, not geometry of type 10242
\end{minted}

The fourth and biggest problem is that queries that combine geo
functions and graph relations take \emph{very} long to compute.  For
testing purposes we have set the limit for query execution to 10,000
seconds, but some queries still do not succeed within that time-frame:

\begin{minted}{text}
  Virtuoso 42000 Error The estimated execution time 12774 (sec)
  exceeds the limit of 10000 (sec).
\end{minted}

We must note that both Virtuoso and StarDog are very good triple
stores overall and that they are working on improving geodata and
GeoSPARQL support.  It is difficult to assess when those improvements
will be good enough for the purpose of setting up a Linked Geodata
service.  It may also be the case that the out-of-the-box experience
that we have tested can be improved through configuration changes that
we are unaware of.


\section{Second strategy: use a SotA Information Retrieval solution}

%Linked Data Platform (Kadaster)
We have corroborated our findings about the deficiency of existing
triple stores with other developers.  A common approach is to perform
graph queries in a triple store and geospatial queries in a document
store such a Solr.  This approach results in a good performance and
coverage of graph/SPARQL queries as well as good performance and
coverage of geospatial queries.  However, because the results come
form different backends, it is generally not possible to perform a
GeoSPARQL query in which graph and geospatial components are
intertwined.


\section{Users have different needs \& capacities (lesson 1)}

One of the ways in whic multiple groups of users can be addressed is
by offering result set formats that they are familiar with.  Our
demonstrator exposes the following result set formats:

\begin{itemize}

\item GeoJSON

\item JSON-LD 1.0

\item N-Quads 1.1, N-Triples 1.1

\end{itemize}

N-Quads and N-Triples are, implementation-wise, the simplest RDF
serialization formats available.  They are supported by almost every
RDF processor.  Additional RDF serialization formats like Turtle 1.1,
RDF/XML 1.1 and TRiG 1.1 are also widely available and are easy to
add.  We notice that the JavaScript RDF libraries\footnote{For more
  information, see the RDF JavaScript Libraries Group
  (\url{https://www.w3.org/community/rdfjs/}).} that are currently
developed for client-side processing do not support, and will maybe
never support, RDF/XML.


\subsection{Header Dictionary Triples (HDT)}
\label{sec:hdt}

Another format that could be added is Header Dictionary Triples
(HDT)~\cite{Fernandez2013}.  These are currently used to power the
Graph API to allow Basic Graph Pattern queries to be performed
(similar to Linked Data Fragments (LDF)~\cite{Verborgh2014}).  This
format could be interesting for users who want to gather very large
result sets.

Textual serialization formats work well for small results sets.  For
instance, if someone asks the ten nearest monuments then this can
easily be returned in a text-based reply.  However, some users have a
large-scale use case, such as requesting \emph{all} monuments in the
Nederlands (e..g, with the purpose of data visualization) A textual
result set of this size is difficult to process.  With HDT the data is
not only much smaller in size (as with regular compression) but can
also be easily processed by the user.


\subsection{GeoJSON}
\label{sec:geojson}

Another format we expose is GeoJSON.  The GeoJSON format is not yet
fully standardized\footnote{GeoJSON is a proposed RFC standard as of
  August 2016.} and the current RFC~\cite{Butler2016} is not a
standard in the traditional sense of the word, i.e., a definition of
all and only GeoJSON constructs and their meaning.  For instance, the
current RFC does not give a formal grammar but relies on examples to
convey GeoJSON syntax and semantics.

It is not very difficult to implement GeoJSON, which is a very
flexible format that can easily be combine with other JSON formats.
Specifically, we were interested in mixing GeoJSON into JSON-LD
constructs.  Some people have worked on this in 2015 under the name
`geojson-ld'\footnote{See
  \url{https://github.com/geojson/geojson-ld}}, but development in
that direction has stalled.

The biggest hurdle towards integrating GeoJSON and JSON-LD into one
format is that JSON arrays are used in JSON-LD for abbreviated object
term notation.  However, GeoJSON uses JSON arrays to represent
geometries (nested lists of floating point coordinates).
This point will be addressed in JSON-LD 1.1\footnote{See \url{https://github.com/json-ld/json-ld.org/issues/397}}.


\subsection{JSON-LD}
\label{sec:jsonld}

JSON-LD 1.0 support was the most problematic result format to
implement.  It differs from Turtle-based serialization formats in that
it does not translate a graph to a sequence of characters, but it
instead performs transformations between RDF graphs and JSON trees.
As such it has to merry requirements from both paradigms.  In this
sense JSON-LD is similar to RDF/XML which undertakes a graph to/from
tree mapping as well.

JSON-LD is valid JSON, the primary data interchange format for web
programmers.  As such, JSON-LD is a good way of lowering the entry
level for this user group.  The JSON-LD format is relatively cosly to
generate and process, because not all aspects of RDF can be directly
encoded in JSON.  For this a \emph{context} that steers the data
transformation step needs to be defined (while generating) and applied
(while processing).  However, as long as JSON-LD is used for
interchanging smaller chunks of data, the extra processing time is not
an issue.

There are currently JSON-LD implementations for C\#, Go, Java,
JavaScript, PHP, Python, Ruby.  Most notably support for C and C++,
languages, in which many low-level database systems and libraries are
written, is missing.  Sadly, the top C++-based RDF processor, Raptor
(\url{http://librdf.org/raptor/}), states on its web site that
``JSON-LD is not supported - too complex to implement''.

Virtuoso does not support loading JSON-LD files\footnote{See
  \url{https://github.com/openlink/virtuoso-opensource/issues/478}}
and ClioPatria does not support JSON-LD out-of-the-box either.  We
have written a partial implementation for generating JSON-LD and
included it into library plRdf\footnote{See
  \url{https://github.com/wouterbeek/plRdf/blob/master/prolog/jsonld/jsonld_build.pl}}.


\subsection{OGC standards (GML, WFS, WMS)}

While GeoJSON addresses users from the web and geo domain, it may not
address more advanced GIS users who may want to use more comprehensive
formats standardized by the OGC.  These formats include GML, WFS and
WMS.  The cost of integrating these OGC formats into a Linked Data
platform are considerable, because they have their own vocabularies
that need to be mapped to and from RDF.  Luckily, the OGC and W3C are
currently working on integrating their respective standards within the
Spatial Data on the Web Working Group\footnote{See
  \url{https://www.w3.org/2015/spatial/wiki/Main_Page}}.  It is
probably a good idea to wait until that Working Group has come up with
a first (proposed) standard.


\section{Findability (lesson 1a)}

\subsection{Vocabulary overview}

One of the most difficult things for web programmers, when confronted
with a Linked Geodata service, is to find out \emph{what is in the
  data}.  For non-Linked Data services this is usually not a problem:
there is a limited number of entity types and relations that can be
queried.  For instance, a product review site may have users who write
reviews for products.  Products may have companies producing them and
users may themselves have ratings to denote their trust level.  That's
about it.

Linked Data is very different: the monument dataset that we started
out with contains over a hundred unique relationships between entities
belonging to dozens of different types.  And this is only one dataset.
It is inherently difficult to provide an overview of what is inside a
large collection of Linked Data, in the same way in which it is
impossible to provide an overview of what is in a large collection of
the web.  We have not found a satisfactory solution for this problem,
which is widely recognized as a difficult problem in Linked Data.  We
do see several possible solutions but have not implemented them yet:

\begin{itemize}

\item Faceted browsers map (part of) the schema to option lists.  By
  selecting the options lists TODO

\item Hierarchy visualization can be used to display the class and/or
  property hierarchy.  StarDog uses this in their default data
  browser.  Since our datasets currently do not contain hierarchies
  this options would first require a more detailed data model.

\end{itemize}


\subsection{Data overview}

\begin{itemize}

\item Various data visualization techniques have been proposed for
  Linked Data, but many of them only work with small data collections.
  E.g., spring embedding visualizations result in unwieldy visuals
  when they include more than a few thousand nodes.

\end{itemize}


\subsection{Free text search}

Datasets can be very large, so good search functions are required to
let users find the needle in the haystack.


\section{Keep it simple (lesson 1B)}

Simplicity is notoriously difficult to achieve in Linked Data.  The
main reason for this is that simplicity is usually implemented by
optimizing for a particular use case.  Linked Data is about ``the
re-use of information in ways that are unforseen by the
publisher''~\cite{Bernerslee2006}.  However, this does not mean that
exisitng approaches of Linked Geodata publishing cannot be simplified.
We do observe that simplifications in Linked Geodata publishing are
costly to implement.  Simply changing something inside the User
Interface is not enough: there are often conceptual reasons why
certain things are difficult.  We now give an example of a
simplification we were able to implement.


\subsection{Uniform representation of geodata}

It would significantly simpler for the user if geospatial data would
be queried in a uniform and simple way.  However, our source datasets
contain multiple approaches towards representing geospatial data.  The
following pattern is commonly used:

\begin{minted}{turtle}
  entity:x geosparql:defaultGeometry _:a                    .
  _:a      geosparql:asWKT           "POINT(5.46 51.35)"    ;
           rdf:type                  geosparql:Geometry     .
\end{minted}

We believe that the above three statements can be rewritten to a
single statement conveying the same meaning:

\begin{minted}{turtle}
  entity:x def:geometry "POINT(5.46 51.35)"^^def:point .
\end{minted}

This version has the following benefits:

\begin{itemize}

\item No introduction of an unnecessary blank node term (\textt{\_:a}).

\item More explicit type information about the kind of geometry
  (\textt{def:point} i.o. \textt{geosparql:Geometry}).

\end{itemize}

These changes bring about the following usability improvements:

\begin{itemize}
  
\item One less level of nesting simplifies querying.  For instance, we
  need one LDF request to retrieve the geometry of an entity instead
  of two.

\item We can query for geometries of a specific type without having to
  parse geometry values.  For instance, for one of our map views we
  want to show polygons but not points and lines.  We achieve this
  with the following query:

\end{itemize}

\begin{minted}{sparql}
  ?x def:geometry ?y FILTER (datatype(?y) = def:polygon)
\end{minted}


\begin{comment}
\subsection{Who is allowed to do what (lesson 1c)}

All our data was published as Linked Open Data with no authentication.
In general we notice that Linked Data is far more often \emph{read}
than \emph{written}.  Our demonstration system does allow SPARQL
Update requests to be sent, but we have not advertised or used this
functionality within the current research.


\subsection{Each speaks its own language and lives in his own world}
\end{comment}


\section{Data transformation}
\label{sec:conversion}
\label{sec:enrichment}
\label{sec:transformation}

Geodata, as it is currently published, does not contain explicit
links.  Links can be inferred from implicit cues by human domain
experts, but not by automated and domain-independent means.  This
means that implementing lesson 2b (Foster to link everything with
everything) requires a relatively large investment.

The same value sometimes denotes the same thing.  E.g., `Appingedam'
denotes a municipality.  However, Appingedam is sometimes denoted by
`GM0003' (gemeentecode) and occurrences of the same string can denote
different things (e.g., the area of Appingedam changes over time, the
municipality of Appingedam is more than just the area of Appingedam,
and a tourist uses `Appingedam' to denote the center of Appingedam).

%The same is true for other value types, e.g., `1903' may denote a year
%in the Gregorian calendar or the length of a road in meters.  


\subsection{Value unpacking}
\label{sec:grammar}

A single value in the source dataset may describe multiple entities
and relations between them.  For instance the small `buurtcode' string
\textt{BU00030002} denotes three entities and two spatial containment
relations:

\begin{minted}{turtle}
  buurt:00030002 rdf:type        def:Buurt      .
  gemeente:0003  geof:sfContains wijk:000300    ;
                 rdf:type        def:Gemeente   .
  wijk:000300    geof:sfContains buurt:00030002 ;
                 rdf:type        def:Wijk       .
\end{minted}

We call the conversion of a simple value to multiple entities and
relations `value unpacking'.  Domain experts have `packed' meaning
into encodings like \texttt{BU00030002}.  Non-domain experts and
machines cannot `see' the meaning that is inside the package.  In
order to make the meaning widely available we must `unpack' it using a
grammar.  For the above code we have to construct the following
grammar.\footnote{The grammar is written in Augmented Backus-Naur Form
  (ABNF)~\cite{Crocker2008}.}

\begin{minted}{abnf}
buurt         := "BU" gemeente-code wijk-code buurt-code
buurt-code    := DIGIT DIGIT
gemeente      := "GM" gemeente-code
gemeente-code := DIGIT DIGIT DIGIT DIGIT
wijk          := "WK" gemeente-code wijk-code(Wijk).
wijk-code     := DIGIT DIGIT
\end{minted}

When loading the data we have to tell the machine to \emph{parse}
values within a given column, tag or key using the above grammar.  The
machine is then able to automatically construct the encoded entities
and relations for all such values.

Notice that the above has to be done for every column, tag or key.
Moreover, not all values can be parsed in this way.  For instance,
human language can generally not be parsed by a machine processor.


\subsection{Value enrichment}

Besides combining and splitting values, we sometimes find that a value
is incomplete: it does not contain all the information we need in
order to interpret it.

For instance, very many values are encoded in human languages.  In
order to do anything useful with these values we must at least know to
which language they belong.  For this we run automatic language
detection algorithms on the values.  The accuracy for medium-length
strings is known to be very high (Figure~\ref{fig:natlang}

\begin{figure}
  \includegraphics[width=\linewidth]{img/natlang.png}
  \caption{Accuracy of natural language detection in RDF strings for
    the 10 most frequent languages.  Strings of similar length are
    combined in buckets.  The size of buckets increases
    logarithmically.  Figure taken from~\cite{Beek2016c}.}
  \label{fig:natlang}
\end{figure}

The main benefit of running natural language detection on the current
data collection is to distinguish between Dutch and English strings.


\subsection{Value combining}

For example events are often described with a day, month and/or year
column, tag or key:

\begin{minted}{text}
  <EVENT-ID> | 1997 | Aug | 7
\end{minted}

In order for dates to be comparable with one another they have to be
converted to values of XML~Schema Part~2:
Datatypes~1.1~\cite{Peterson2012}.  Month names can be converted using
a simple, one-to-one grammar (Section~\ref{sec:grammar}).  After that
values can be converted manually to datatypes.

\begin{minted}{turtle}
  event:x def:day   "07"^^xsd:gDay    ;
          def:jaar  "1997"^^xsd:gYear ;
          def:maand "8"^^xsd:gMonth   .
\end{minted}

Values can now be automatically combined into the following more
complex datatype:

\begin{minted}{turtle}
  event:x def:date "1997-08-07"^^xsd:date .
\end{minted}

Because support for XML Schema Datatypes is very good overall, dates,
times, durations, lengths, weights, etc. can now be compared through
built-in functions.  For instance the function
\textt{op:dateTime-less-than} is used by SPARQL 1.1 implementations to
directly compare an event to all events that happened before
it~\cite{Malhotra2015}.



\section{Persistent IRIs (lesson 2c)}

It is very difficult to ensure persistent IRIs.

Most datasets have no IRIs at all, so we have to mint IRIs for each
entity.  While enriching the data (Section~\ref{sec:enrichment}) the
number of entities increases.  These requires IRIs as well.

We have found the Dutch Linked Data URI strategy to be very beneficial
during this process.  It distinguishes between IRIs for instance
(ABOX) and schema (TBOX) entities.

\begin{comment}
Publish data in registries at unique persistent URIs to ensure links
between data sets are available, attainable and sustainable in the
future.

Using URIs, you can link to something in a unique way and therefore
uniquely distinguished. You need to use stable HTTP URIs for spatial
objects and datasets in order to keep the links future-proof and
reliable.  Intended outcome

Registries, and especially the base registries, have proper uri
management. Records are made available on a unique persistent uri,
which enables the linking of objects within these registries to each
other.  Possible approach

Publishing data records on unique persistent URIs requires a URI
strategy. For example using the Dutch Linked Data URI Strategy:

URI:

    /id/{resource} 303 redirect to /doc/{resource}
    /doc/{resource}

Example: https://geo4web.apiwise.nl/gemeente/GM0307

URI CBS Amersfoort
How to test

Several flavors exist but we recommend to the use Dutch URI Strategy
as it was indexed by Google relatively fast. Read more here
\end{comment}


\begin{comment}
\section{Motivation for applying for this research topic}

Triply applies for this research topic because it sees innovation
potential for the way geodata is currently published on the Web of
Data.  Triply wants to explore and concretely deliver significant
improvements to the way in which geodata is disseminated to data
consumers.  Triply believes that it is possible to unlock innovation
potential within this tender given (1) the close collaboration with
Geonovum, (2) the presence of other data suppliers such as PDOK and
cultural heritage organizations, and (3) the lessons learned during
the previous tender round on which we can build.

When we look at contemporary geodata publishing practices we see two
worlds colliding: On the one hand there are data publishers that use
OGC standards that are very expressive but also very complex.  On the
other hand there is the Web of heterogeneous and distributed data
sources.  There is a large number of (potential) data consumers that
are using the contemporary Web stack and are used to far simpler and
more Open Web APIs.  (These are the two parties that are also
identified in the research question for this research task.)

Luckily, usability and complexity need not exclude each another once
the Linked Data approach is applied correctly.  Since Linked Data does
not require a single database schema to operate, we have the ability
to disseminate the same data through different but complementary
vocabularies.  For instance, there can be two schemas: one complex and
difficult and another simplistic and easy (lesson 1B).  In this
example the two vocabularies cater towards the aforementioned two user
groups.  In this way the same data can be delivered in different ways,
benefiting different groups (lesson 1).  While the specification of
multiple vocabularies/schemas on top of the same data collection comes
at a low cost it does provide the necessary tuning to user
capabilities (lesson 1D).

The other part of the research question targets the ``constraints
(budgets) and capabilities (in-house know-how)'' at the data publisher
side.  Operating costs can be split into fixed and variable costs.
The \textbf{fixed cost} of deploying a Linked Data solution (e.g. a
remotely accessible triple store) is considerable.  At the moment
governmental organizations wish to use and/or provide a Linked Data
deployment they oftentimes have in-house people that do a considerable
portion of the engineering.  There is no clear reason why Linked Data
deployment should require more in-house knowledge then using the
electric grid.  With the Triply Linked Data product a (governmental)
organization uses the Triply Linked Data infrastructure in a similar
way in which it uses the electric grid; it simply connects.  Because
Triply significantly reduces the fixed cost of deploying a Linked Data
solution, the (governmental) organization can focus on curating the
data and extracting value from it.

The \textbf{variable cost} of deploying a Linked Data solution is an
entirely different story.  While it is somewhat difficult to set up a
triple store it is virtually impossible to build one that scales to
service a large amount of (consecutive) users.  When we look at the
SPARQL endpoint observatory SPARQLes ({\small
  \url{http://sparqles.ai.wu.ac.at}})~[\cite{BuildAranda2013}], we see
that 80\% of current SPARQL endpoints have low availability.  When we
look at the 20\% SPARQL endpoints with high availability, we see that
they enforce restrictions on the kinds of questions that can be asked
and the size of answer sets that are returned.  This effectively means
that remote triple stores are either unavailable or severely
constrained.  No Web programmer wants to program against a database
with either of these properties.  Triply is the first Linked Data
solution that allows the number of (consecutive) users to scale
without blowing up the variable cost of endpoint operation.  Triply is
able to do this because Triply is based on recent innovations in
large-scale and scalable Linked Data deployments: Linked Data Fragments
(LDF)~[\cite{Verborgh2014}], Header Dictionary Triples
(HDT)~[\cite{Fernandez2013}] and LOD Laundromat~[\cite{Beek2014}].

\section{Plan of approach}

The plan of approach focuses on realizing maximal impact within a
short period of time.  All crucial decisions in the plan of approach
will be made by Triply's CTO who has extensive experience with
building large-scale data systems.  Some of the low level tasks in the
plan of approach will be performed by a Triply engineer.

\subsection{Requirements analysis}

Triply will collaborate with Geonovum, data providers and task 2
partners in order to make a requirements analysis that specifies what
is needed in order to support the goals of the overarching tender
call.  Making the requirements analysis a shared responsibility will
ensure that development of the demonstrator will be demand-driven.

\subsection{Data conversion and cleaning}

Since some data may either not yet be formatted as RDF or may not yet
be fully standards-compliant, Triply will perform data conversion and
cleaning steps in order to ensure that all data meets certain quality
criteria.  Inherent bugs in the data that cannot be fixed by
technological means alone will be communicated back to the original
data suppliers.

\subsection{Database layer}

The result of the data conversion and cleaning steps will be populated
into a SotA database backend.  The backend will be chosen so as to be
able to service at least one thousand simultaneous API users.  The
focus is here on features that Web developers find beneficial, i.e.,
prioritizing scalability, efficiency and performance (lesson 1B).

\subsection{API layer}

Triply will define and implement a RESTful Web API that follows the
lessons learned, together with formal and de facto standards in Web
API construction.  This RESTful API layer will focus on lowering the
barrier for (Web) programmers to use Geodata.  In line with lesson 3A,
content negotiation will be used to let a data consumer specify the
format s/he prefers.

%Since the Web programmer wants to build an application quickly and
%may not know each and every technology out there, Triply will ensure
%that data can be accessed in each of the currently standardized
%Linked Open Data formats: RDF/XML, N-Triples, N-Quads, TriG, JSON-LD,
%RDFa, Schema.org, IRI dereferencing, Linked Data Fragments (LDF),
%Linked Data Platform (LDP), SPARQL and GeoSPARQL.

%Following lesson 3A this will be done (1) terms of HTTP Accept
%headers and (2) in terms of content-specific URIs (to the extent that
%this does not conflict with the URI persistency requirement).

\subsection{Iteration cycle}

Once the whole system is fully functional and has task 2 partners
programming against it, Triply will assess which changes are needed in
each of the layers in order to improve the demonstrator for maximal
utility.

\section{Results}

This section enumerates the results that will be made available.

\subsection{Data}

Triply will make all cleaned, converted, linked and otherwise enriched
data available to the original data suppliers.  The data will contain
future-proof persistent URIs (lesson 2C) and will contain structured
annotations that make it easy to find when published online (lesson
2D).  Persistent URIs will be based on the Dutch Linked Data URI
Strategy~\cite{Overbeek2013}.

\subsection{Vocabulary}

Triply will make the vocabulary that bridges the geodata and Web API
communities available to Geonovum and/or the original data suppliers.

\subsection{Demonstrator}

Triply will provide a demonstrator Web Service that will be used by
task 2 collaborators.  Data in the demonstrator will be supplied in
standardized Linked Open Data formats (lesson 3A).

\subsection{Reports}

Triply will write a report in which it will validate the lessons
learned with respect to the practice of making large quantities of
geodata available in a Web- and developer-friendly way.

\subsection{Dissemination}

Finally, Triply will communicate the results obtained within the
Geonovum tender with the wider Linked Open Data, Geodata and Web API
communities.  Concretely, Triply will perform the following
dissemination activities:

\begin{enumerate}[noitemsep,nolistsep]
  
\item Triply will, in collaboration with partners, present the results
  of the Geonovum tender at community events about Linked Open Data,
  Geodata, and Geo-oriented Web APIs.
  
\item Triply will write a report describing the tasks that have been
  performed.  The report will also contain the findings and answers to
  the task description questions (as enumerated in the ``Invitation to
  tender'' document).

\item Triply will take the lead in writing a white paper about the
  requirements for the deployment of a scalable Linked Geodata backend
  and an approachable Web API for Open Geodata.  The paper will be
  written in close collaboration with the various partners and will be
  submitted to an appropriate venue (e.g., WWW, ESWC Industry Track or
  Semantics).
  
\end{enumerate}


\subsection{References}

Triply is a startup that is based on research previously conducted by
the Knowledge Representation and Reasoning (KR\&R) group at VU
University Amsterdam (VUA).  Headed by Prof. Dr. Frank van Harmelen,
this is the top research group in the world in the area of Semantic
Web and Linked Data research.

The founders of Triply have proven their ability to \textbf{conduct
  outstanding research} in the field of Semantic Web and Linked Data.
This has resulted in multiple highly cited research papers over the
last two years alone.  The Triply founder's research effort has
culminating in winning the Best Research Paper Award at the
International Semantic Web Conference (ISWC) 2015, which is the top
venue for Semantic Web research.  Here follows a selection of relevant
publications over the last two years:

In addition to delivering outstanding research, the founders of Triply
have proven their ability to \textbf{build outstanding Linked Data
  solutions} as well.  Firstly, the LOD Laundromat
(\url{http://lodlaundromat.org}) ecosystem has won the Best Dutch
Linked Open Data Award 2015 (and ended $3^{rd}$ in the European
competition).  LOD Laundromat is widely recognized as a Game Changer
within the Semantic Web research community.  Secondly, Triply’s CTO
has build Yasgui (\url{http://yasgui.org}), the most feature-rich
SPARQL editor.  Yasgui is used by many high-impact Linked Data
projects and parties such as Sesame, Jena, Smithsonian Museum, German
National Library of Economics and Linked Open Vocabularies.  Thirdly,
Triply’s CEO is involved in the development of the ClioPatria triple
store (\url{http://cliopatria.swi-prolog.org}) and SWI-Prolog’s
Semantic Web library (\url{http://www.swi-prolog.org}).  Finally, the
founders of Triply have recently been involved in the construction of
LOTUS (\url{http://lotus.lodlaundromat.org/}), a large-scale and
customizable Semantic Web search engine.


\subsection{Indication of in-kind investment}

Triply will match Geonovum’s investment amount with an in-kind
investment of equal size, i.e., \euro{}10.000,- The in-kind investment
is necessary to realize the ambitions expressed in the plan of
approach.


\subsection{Statement of agreement}

Triply agrees to publish all research results and deliverables that
flow from this tender under the CC/by license.
\end{comment}


\section{Quantifying the effort}

There were also some `hidden costs', i.e., either things that we
expected to be very simple but that turned out to be very difficult,
or things that we did not expect in the first place.

Firstly, we had to spend an enormous amount of time converting the
various data formats (Section~\ref{sec:conversion}).  After the
conversion, we had to go through several iterations of transforming
the data to improve its quality.  While our transformed data has a
much higher quality than the source data we started out with, we
believe that there are still several more iterations needed to get the
data to a quality level that allows generic (SPARQL) queries to be
performed painlessly (i.e., without ad-hoc string manipulation).

Secondly, we expected to find a plethora of tools that allow Linked
Data to be exposed to web programmers.  After all, Linked Data is web
data.  We were very surprised to find that this is not the case at
all.  In fact, there were many perfectly valid requests by the web
programmers we worked with that could not be easily implemented by
integrating some existing library.


\begin{comment}
\section{Conclusion}

We conclude that it is not yet possible to go from a loose collection
of (geo)data sources to a usable web service for web programmers
within two months.  There are too many missing pieces and the cost of
data conversion and transformation is simply too high.

However, we do believe that the lessons learned in previous research
and validated in our research do point to a.
\end{comment}


\subsection{Shortcuts}

We believe that it is possible to speed up and reduce the cost of the
process of developing a geospatial Linked Data web service by taking
`shortcuts'.


\paragraph{Points only}

Many of the deficiencies of SotA triple stores can be worked around by
reducing all geometries to 2D points.  GeoSPARQL functions between
points are mostly supported.  The downside to this is that much of the
geospatial information is lost and very many geospatial query
(contains, intersects) cannot be performed at all.  This also do not
solve the problem that some queries never terminate or that many
queries take too long.


\paragraph{Start with Linked Data}

In this research project we had to split development costs between
implementing a web service and converting/transforming data.  By
starting out with Linked Data a big chunk of the costs can be saved
(approx. 40\% in our case).


\bibliographystyle{plain}
\bibliography{main}

\appendix

\section{Used aliases}

\begin{table}
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Alias} & \textbf{IRI prefix}\\
    \hline
    \hline
    \textt{geo}    & \url{http://www.opengis.net/ont/geosparql#}\\
    \hline
    \textt{bif}    & \url{http://www.openlinksw.com/schemas/bif#}\\
    \hline
    \textt{rdf}    & \url{http://www.w3.org/1999/02/22-rdf-syntax-ns#}\\
    \hline
    \textt{rdfs}   & \url{http://www.w3.org/2000/01/rdf-schema#}\\
    \hline
  \end{tabular}
  \caption{Aliases for commonly occurring IRI prefixes.}
  \label{tab:alias}
\end{table}
  
\end{document}
